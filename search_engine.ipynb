{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2020 22:44:59 - INFO - faiss -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "# General libraries\n",
    "import re, os, string, random, requests\n",
    "import pandas as pd\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "# Haystack importings\n",
    "from haystack import Finder\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.utils import print_answers\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting ElasticSearch server as daemon\n",
    "es_server = Popen(['elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT  # as daemon\n",
    "                  )\n",
    "\n",
    "# wait until ElasticSearch has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(n):\n",
    "    \"\"\"Return a random string of length n\"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    result_str = ''.join(random.choice(letters) for i in range(n))\n",
    "    return result_str\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc(doc):\n",
    "    \"\"\"Trim doc with respect to the boundary of a sentence.\"\"\"\n",
    "    \n",
    "    trimmedText = []\n",
    "    charCount = 0\n",
    "    for sentence in doc.split('.'):\n",
    "        if charCount < DOC_THRESHOLD:\n",
    "            charCount+=len(sentence.strip())\n",
    "            trimmedText.append(sentence)\n",
    "\n",
    "    finalText = \".\".join(trimmedText)\n",
    "    \n",
    "    return finalText\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Doc cleaning\"\"\"\n",
    "    \n",
    "    # Lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = \"\".join([c for c in text if c not in PUNCTUATION])\n",
    "    \n",
    "    # Removing whitespace and newlines\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    # Trimming doc\n",
    "    text = trim_doc(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ES_INDEX = get_index(10) # Elastic Search DB index name\n",
    "PUNCTUATION = \"\"\"!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\"\"\" # excluding . (full-stop) from the set of punctuations\n",
    "DOC_THRESHOLD = 10000 # character limit for a doc\n",
    "TOP_K_RETRIEVER = 10 # top k documents to analyze further for a given query\n",
    "TOP_K_READER = 5 # top k number of answers to return\n",
    "TOP_K_KEYWORDS = 10 # top k number of keywords to retrieve in a ranked document\n",
    "BASE_URL = \"http://localhost:9200/\"+ES_INDEX+\"/_doc/\"\n",
    "STOPWORD_PATH = \"data/stopwords.txt\"\n",
    "question = \"What is ROC curve?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and Its Applications</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-and-its-applications.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Arti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-cortex-and-its-application-to-a...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\\nAND ITS APPLICATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Term Potentiation and Depression ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long-term-potentiation-and-depress...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\nLONG?TERM POTENTIATION AND DEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-network-models.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwork Models\\nGerhard Paass\\nJorg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, and Active Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation-and-active-learning.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year  \\\n",
       "0     1  1987   \n",
       "1    10  1987   \n",
       "2   100  1988   \n",
       "3  1000  1994   \n",
       "4  1001  1994   \n",
       "\n",
       "                                                                             title  \\\n",
       "0                   Self-Organization of Associative Database and Its Applications   \n",
       "1  A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Arti...   \n",
       "2  Storing Covariance by the Associative Long-Term Potentiation and Depression ...   \n",
       "3                            Bayesian Query Construction for Neural Network Models   \n",
       "4                  Neural Network Ensembles, Cross Validation, and Active Learning   \n",
       "\n",
       "  event_type  \\\n",
       "0        NaN   \n",
       "1        NaN   \n",
       "2        NaN   \n",
       "3        NaN   \n",
       "4        NaN   \n",
       "\n",
       "                                                                          pdf_name  \\\n",
       "0             1-self-organization-of-associative-database-and-its-applications.pdf   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-cortex-and-its-application-to-a...   \n",
       "2  100-storing-covariance-by-the-associative-long-term-potentiation-and-depress...   \n",
       "3                   1000-bayesian-query-construction-for-neural-network-models.pdf   \n",
       "4           1001-neural-network-ensembles-cross-validation-and-active-learning.pdf   \n",
       "\n",
       "           abstract  \\\n",
       "0  Abstract Missing   \n",
       "1  Abstract Missing   \n",
       "2  Abstract Missing   \n",
       "3  Abstract Missing   \n",
       "4  Abstract Missing   \n",
       "\n",
       "                                                                        paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisa...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\\nAND ITS APPLICATION...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\nLONG?TERM POTENTIATION AND DEP...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwork Models\\nGerhard Paass\\nJorg ...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders K...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/papers.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structuring data to haystack required format\n",
    "# Format: [{'text': 'paper_content', 'meta':{'name':'title'}}]\n",
    "docs = []\n",
    "corpora = []\n",
    "doc_len = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    dicts = {}\n",
    "    dicts['text'] = clean_text(row['paper_text'])\n",
    "    doc_len.append(len(dicts['text']))\n",
    "    corpora.append(dicts['text'])\n",
    "    dicts['meta'] = {}\n",
    "    dicts['meta']['name'] = clean_text(row['title'])\n",
    "    docs.append(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10245.576577820742"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average characters in a document after trimming\n",
    "sum(doc_len)/len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2020 22:46:03 - INFO - elasticsearch -   PUT http://localhost:9200/oczbuiemph [status:200 request:0.880s]\n",
      "11/27/2020 22:46:03 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.003s]\n"
     ]
    }
   ],
   "source": [
    "# Be careful while overwriting data on the same ES index\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=ES_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2020 22:46:04 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.327s]\n",
      "11/27/2020 22:46:05 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.162s]\n",
      "11/27/2020 22:46:07 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.106s]\n",
      "11/27/2020 22:46:08 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.119s]\n",
      "11/27/2020 22:46:09 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.098s]\n",
      "11/27/2020 22:46:10 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.134s]\n",
      "11/27/2020 22:46:11 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.109s]\n",
      "11/27/2020 22:46:13 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.124s]\n",
      "11/27/2020 22:46:14 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.106s]\n",
      "11/27/2020 22:46:15 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.118s]\n",
      "11/27/2020 22:46:16 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.074s]\n",
      "11/27/2020 22:46:17 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.074s]\n",
      "11/27/2020 22:46:18 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.095s]\n",
      "11/27/2020 22:46:19 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.127s]\n",
      "11/27/2020 22:46:21 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.082s]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating ES retriever \n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2020 22:46:21 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "11/27/2020 22:46:21 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/27/2020 22:46:27 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "11/27/2020 22:46:47 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "11/27/2020 22:46:47 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...\n",
      "11/27/2020 22:46:47 - INFO - farm.infer -    0    0    0    0    0    0    0 \n",
      "11/27/2020 22:46:47 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "11/27/2020 22:46:47 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n",
      "11/27/2020 22:46:47 - INFO - farm.infer -               \n"
     ]
    }
   ],
   "source": [
    "# Initializing reader on the top of roberta-base-squad2 pre-trained model, which will be downloaded on the first run\n",
    "# Here, we can set the size of context window for our answers and use the GPU if available\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\",use_gpu=False, context_window_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting reader and retriever to Finder\n",
    "finder = Finder(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/27/2020 22:46:47 - INFO - elasticsearch -   POST http://localhost:9200/oczbuiemph/_search [status:200 request:0.071s]\n",
      "11/27/2020 22:46:47 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "11/27/2020 22:46:47 - INFO - haystack.finder -   Reader is looking for detailed answer in 102477 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.62s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.52s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.78s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.98s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.21s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.83s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.49s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.88s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.26s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.87s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "# Question prediction with TOP_K_RETRIEVER and TOP_K_READER\n",
    "prediction = finder.get_answers(question=question, top_k_retriever=TOP_K_RETRIEVER, top_k_reader=TOP_K_READER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'answer': 'a straight line connecting the origin to 1 1',\n",
      "        'context': 'tion of the false positive rate. the points of the curve '\n",
      "                   'are obtained by sweeping the classification threshold from '\n",
      "                   'the most positive classification value to the most '\n",
      "                   'negative. for a fully random classification the roc curve '\n",
      "                   'is a straight line connecting the origin to 1 1. any '\n",
      "                   'improvement over random classification results in an roc '\n",
      "                   'curve at least partially above this straight line. fig. 1 '\n",
      "                   'shows an example of roc curve. the auc is defined as the '\n",
      "                   'area under the roc curve and is closely related to'},\n",
      "    {   'answer': 'a piecewise linear function',\n",
      "        'context': 'not assume the classifiers are independent or related in '\n",
      "                   'any way. before introducing our method we analyze the '\n",
      "                   'oneclassifier case n 1. lemma 1 let f1 be a classifier '\n",
      "                   'with performance probabilities pd1 and pf 1 . its optimal '\n",
      "                   'roc curve is a piecewise linear function parameterized by '\n",
      "                   'a free parameter bounding pf for pf 1 pd pd1 pf 1 and for '\n",
      "                   'pf 1 pd 1 pd1 1 pf 1 pf 1 pd1 . proof. when pf 1 we can '\n",
      "                   'obtain a likelihood ratio test by setting 1 and pf 1 and '\n",
      "                   'for pf 1 we set 0 and pf 1 1 pf 1 . 2 2 the int'},\n",
      "    {   'answer': 'a plot of h vs. f parameterized by the thresholdy',\n",
      "        'context': ' the hit rate h is the fraction of familiar targets the '\n",
      "                   'network correctly identifies as familiar and the false '\n",
      "                   'alarm rate f is the fraction of unfamiliar targets the '\n",
      "                   'network incorrectly identifies as familiar. an roc curve '\n",
      "                   'is a plot of h vs. f parameterized by the thresholdy i.e. '\n",
      "                   'it is equivalent to the two curves fh and hh . the area '\n",
      "                   'under the roc curve is the cindex a measure of predictive '\n",
      "                   'accuracy that is independent of both the fraction of '\n",
      "                   'positive familiar cases in the test set and the posi'},\n",
      "    {   'answer': 'a straight line connecting the origin to 1 1',\n",
      "        'context': 'tion of the false positive rate. the points of the curve '\n",
      "                   'are obtained by sweeping the classification threshold from '\n",
      "                   'the most positive classification value to the most '\n",
      "                   'negative. for a fully random classification the roc curve '\n",
      "                   'is a straight line connecting the origin to 1 1. any '\n",
      "                   'improvement over random classification results in an roc '\n",
      "                   'curve at least partially above this straight line. the auc '\n",
      "                   'is defined as the area under the roc curve. consider a '\n",
      "                   'binary classification task with m positive examples'},\n",
      "    {   'answer': 'the area under a roc curve',\n",
      "        'context': 'g above all others is the best possible choice. from a '\n",
      "                   'statistical learning perspective risk minimization or '\n",
      "                   'performance maximization strategies for bipartite ranking '\n",
      "                   'have been based mostly on a popular summary of the roc '\n",
      "                   'curve known as the area under a roc curve auc see clv08 '\n",
      "                   'fiss03 agh 05 which corresponds to the l1 metric on the '\n",
      "                   'space of roc curves. in the present paper we propose a '\n",
      "                   'statistical methodology to estimate the optimal roc curve '\n",
      "                   'in a stronger sense than the auc sense namely in the '}]\n"
     ]
    }
   ],
   "source": [
    "# Printing answers with minimal detail\n",
    "# details = minimal | medium | all\n",
    "\n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_docs = []\n",
    "\n",
    "for doc in prediction['answers']:\n",
    "    DOC_URL = BASE_URL + doc['document_id']\n",
    "    response = requests.get(DOC_URL)\n",
    "    if response.status_code == 200:\n",
    "        full_doc = {}\n",
    "        full_doc['title'] = response.json()['_source']['name']\n",
    "        full_doc['text'] = response.json()['_source']['text']\n",
    "        full_doc['answer'] = doc['answer']\n",
    "        top_5_docs.append(full_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top K keywords using TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowhit/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "#load a set of stop words\n",
    "stopwords=get_stop_words(STOPWORD_PATH)\n",
    "\n",
    "# Initializing TF-IDF Vectorizer with stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, smooth_idf=True, use_idf=True)\n",
    "\n",
    "# Creating vocab with our corpora\n",
    "vectorizer.fit_transform(corpora)\n",
    "\n",
    "# Storing vocab\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only TOP_K_KEYWORDS\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,TOP_K_KEYWORDS)\n",
    "    \n",
    "    return list(keywords.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in top_5_docs:\n",
    "    doc['keywords'] = get_keywords(vectorizer, feature_names, doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(top_5_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ROC curve?\n",
      "Top 5 articles with keywords\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc optimization vs. error rate minimization</td>\n",
       "      <td>auc optimization vs. error rate minimization corinna cortes and mehryar mohr...</td>\n",
       "      <td>a straight line connecting the origin to 1 1</td>\n",
       "      <td>[auc, roc, examples, rate, positive, negative, classification, threshold, cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>optimal roc curve for a combination of classifiers</td>\n",
       "      <td>optimal roc curve for a combination of classifiers marco barreno alvaro a. c...</td>\n",
       "      <td>a piecewise linear function</td>\n",
       "      <td>[roc, pf, classifiers, curve, pd1, neymanpearson, pd, yh0, pry, h0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>familiarity discrimination of radar pulses</td>\n",
       "      <td>familiarity discrimination of radar pulses eric grangerl stephen grossberg 2...</td>\n",
       "      <td>a plot of h vs. f parameterized by the thresholdy</td>\n",
       "      <td>[familiarity, artmapfd, artmap, discrimination, fuzzy, radar, training, fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confidence intervals for the area under the roc curve</td>\n",
       "      <td>confidence intervals for the area under the roc curve corinna cortes google ...</td>\n",
       "      <td>a straight line connecting the origin to 1 1</td>\n",
       "      <td>[auc, confidence, intervals, roc, variance, negative, positive, examples, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>overlaying classifiers a practical approach for optimal ranking</td>\n",
       "      <td>overlaying classifiers a practical approach for optimal ranking stephan clem...</td>\n",
       "      <td>the area under a roc curve</td>\n",
       "      <td>[roc, scoring, curve, ranking, optimal, bipartite, curves, risk, sx, positive]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             title  \\\n",
       "0                     auc optimization vs. error rate minimization   \n",
       "1               optimal roc curve for a combination of classifiers   \n",
       "2                       familiarity discrimination of radar pulses   \n",
       "3            confidence intervals for the area under the roc curve   \n",
       "4  overlaying classifiers a practical approach for optimal ranking   \n",
       "\n",
       "                                                                              text  \\\n",
       "0  auc optimization vs. error rate minimization corinna cortes and mehryar mohr...   \n",
       "1  optimal roc curve for a combination of classifiers marco barreno alvaro a. c...   \n",
       "2  familiarity discrimination of radar pulses eric grangerl stephen grossberg 2...   \n",
       "3  confidence intervals for the area under the roc curve corinna cortes google ...   \n",
       "4  overlaying classifiers a practical approach for optimal ranking stephan clem...   \n",
       "\n",
       "                                              answer  \\\n",
       "0       a straight line connecting the origin to 1 1   \n",
       "1                        a piecewise linear function   \n",
       "2  a plot of h vs. f parameterized by the thresholdy   \n",
       "3       a straight line connecting the origin to 1 1   \n",
       "4                         the area under a roc curve   \n",
       "\n",
       "                                                                          keywords  \n",
       "0  [auc, roc, examples, rate, positive, negative, classification, threshold, cu...  \n",
       "1              [roc, pf, classifiers, curve, pd1, neymanpearson, pd, yh0, pry, h0]  \n",
       "2  [familiarity, artmapfd, artmap, discrimination, fuzzy, radar, training, fami...  \n",
       "3  [auc, confidence, intervals, roc, variance, negative, positive, examples, ra...  \n",
       "4   [roc, scoring, curve, ranking, optimal, bipartite, curves, risk, sx, positive]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(question)\n",
    "print(\"Top 5 articles with keywords\\n\")\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
